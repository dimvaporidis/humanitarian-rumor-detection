{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimitris/anaconda3/envs/ztdl/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report,confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import ast\n",
    "import pickle\n",
    "from sklearn import tree\n",
    "import pydot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the first dataset\n",
    "X = pd.read_csv('normalized_generic.csv',sep='\\t')\n",
    "X = X.drop('Unnamed: 0',axis=1)\n",
    "\n",
    "\n",
    "# Merging the two datasets\n",
    "a=X.loc[X['rumor'] == 0.0]\n",
    "b=X.loc[X['rumor'] == 1.0]\n",
    "a=a[0:len(b)]\n",
    "X=pd.concat([a,b])\n",
    "X = shuffle(X)\n",
    "X= X.reset_index(drop=True)\n",
    "\n",
    "# Defining the last column of the dataframe as the output column of the model\n",
    "y=X['rumor']\n",
    "\n",
    "X = X.iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metacontent_features= X.iloc[:,:8]\n",
    "metacontent_features['rumor']=X.rumor\n",
    "metacontent_features.to_csv('metacontent_features.csv', sep='\\t')\n",
    "user_features= X.iloc[:,8:32]\n",
    "user_features['rumor']=X.rumor\n",
    "user_features.to_csv('user_features.csv', sep='\\t')\n",
    "linguistic_features= X.iloc[:,32:]\n",
    "linguistic_features.to_csv('linguistic_features.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=X.loc[X['rumor'] == 0.0]\n",
    "b=X.loc[X['rumor'] == 1.0]\n",
    "a=a[0:len(b)]\n",
    "X=pd.concat([a,b])\n",
    "X = shuffle(X)\n",
    "X= X.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rumor Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desicion Tree Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [ 2, 4, 6, 8, 10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4, 8]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state = 42)\n",
    " \n",
    "# Define multiple metrics to be stored during the GridSearch\n",
    "scoring = {'accuracy':'accuracy','precision':'precision', 'f1':'f1', 'recall':'recall'}\n",
    "\n",
    "# Random search of parameters, using 5 fold cross validation\n",
    "dt_random_truth = GridSearchCV(estimator=dt, param_grid=random_grid, cv=5, scoring= scoring, \n",
    "                               n_jobs=8, refit='recall')\n",
    "\n",
    "# Fit the random search model\n",
    "dt_random_truth.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_truth_dt = open('dt_rumour.pkl', 'wb')\n",
    "\n",
    "pickle.dump(dt_random_truth.cv_results_, output_truth_dt)\n",
    "\n",
    "output_truth_dt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the values of C in logarithmic progression\n",
    "Cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "\n",
    "# Exploring the values of Gamma in logarithmic progression\n",
    "gammas = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "# Exploring the most suitable kernel\n",
    "kernel = ['rbf', 'linear']\n",
    "\n",
    "param_grid = {'C': Cs, 'gamma' : gammas, 'kernel': kernel}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model\n",
    "svm = SVC(random_state = 42)\n",
    "\n",
    "# Define multiple performance metrics to be stored in the GridSearch\n",
    "scoring = {'accuracy':'accuracy','precision':'precision', 'f1':'f1', 'recall':'recall'}\n",
    "\n",
    "# Random search of parameters, using 5 fold cross validation \n",
    "grid_search_truth_svm = GridSearchCV(estimator=svm, param_grid=random_grid, cv=5, scoring= scoring, \n",
    "                                     pre_dispatch=8, refit='recall')\n",
    "\n",
    "# Fit the model\n",
    "grid_search_truth_svm.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_truth_svm = open('svm_rumour.pkl', 'wb')\n",
    "\n",
    "pickle.dump(grid_search_truth_svm.cv_results_, output_truth_svm)\n",
    "\n",
    "output_truth_svm.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Humanitarian Relevancy Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = pd.read_csv('generic_text_all.csv',sep='\\t', header=None, index_col=False, lineterminator='\\n')\n",
    "X1 = X1.drop(0,axis=1)\n",
    "X1 = X1.drop(0)\n",
    "X1 = X1.reset_index(drop=True)\n",
    "X1['crisis'] = 0.0 \n",
    "\n",
    "X2 = pd.read_csv('humanitarian_text.csv',sep='\\t', header=None, index_col=False, lineterminator='\\n')\n",
    "X2 = X2.drop(0,axis=1)\n",
    "X2 = X2.reset_index(drop=True)\n",
    "X2['crisis'] = 1.0\n",
    "\n",
    "\n",
    "X=pd.concat([X1, X2])\n",
    "X= X.reset_index(drop=True)\n",
    "y = X['crisis']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.sparse.save_npz('unigram.npz', messages_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matrix = sp.sparse.load_npz('bigram.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "fit = [True, False]\n",
    "\n",
    "parameters = {'alpha': a, 'fit_prior': fit}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimitris/anaconda3/envs/ztdl/lib/python3.5/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/home/dimitris/anaconda3/envs/ztdl/lib/python3.5/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/home/dimitris/anaconda3/envs/ztdl/lib/python3.5/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/home/dimitris/anaconda3/envs/ztdl/lib/python3.5/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/home/dimitris/anaconda3/envs/ztdl/lib/python3.5/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/home/dimitris/anaconda3/envs/ztdl/lib/python3.5/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/home/dimitris/anaconda3/envs/ztdl/lib/python3.5/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/home/dimitris/anaconda3/envs/ztdl/lib/python3.5/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/home/dimitris/anaconda3/envs/ztdl/lib/python3.5/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/home/dimitris/anaconda3/envs/ztdl/lib/python3.5/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "       fit_params=None, iid=True, n_jobs=8,\n",
       "       param_grid={'alpha': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], 'fit_prior': [True, False]},\n",
       "       pre_dispatch='2*n_jobs', refit='recall', return_train_score=True,\n",
       "       scoring={'f1': 'f1', 'precision': 'precision', 'recall': 'recall', 'accuracy': 'accuracy'},\n",
       "       verbose=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "scoring = {'accuracy':'accuracy','precision':'precision', 'f1':'f1', 'recall':'recall'}\n",
    "grid_search2 = GridSearchCV(mnb, parameters, cv=5, scoring=scoring, n_jobs= 8, refit='recall',return_train_score=True)\n",
    "grid_search2.fit(sparse_matrix,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "output1 = open('mnb_unigram.pkl', 'wb')\n",
    "\n",
    "pickle.dump(grid_search2.cv_results_, output1)\n",
    "\n",
    "output1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of features to consider at every split\n",
    "#max_features = ['auto', 'sqrt']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [10,100,1000,10000]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 4, 6, 8, 10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4, 8]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state = 42)\n",
    "# Random search of parameters, using 5 fold cross validation, \n",
    "scoring = {'accuracy':'accuracy','precision':'precision', 'f1':'f1', 'recall':'recall'}\n",
    "dt_random = GridSearchCV(estimator=dt, param_grid=random_grid, cv=5, scoring= scoring, \n",
    "                                     n_jobs=4, refit='accuracy')\n",
    "\n",
    "# Fit the random search model\n",
    "dt_random.fit(sparse_matrix,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_random.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output3 = open('dt_unigram_only.pkl', 'wb')\n",
    "\n",
    "pickle.dump(dt_random.cv_results_, output3)\n",
    "\n",
    "output3.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
