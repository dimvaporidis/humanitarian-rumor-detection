{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from geotext import GeoText\n",
    "from sklearn import preprocessing\n",
    "import tweepy\n",
    "from tweepy import API\n",
    "import datetime\n",
    "import urllib\n",
    "import face_recognition\n",
    "from difflib import SequenceMatcher\n",
    "import gender_guesser.detector as gender\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiating Connection to Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key=\"I8ZQyd1mWDFdq6yPC4EaMhuUJ\"\n",
    "consumer_secret=\"s4d9EL27YFzPikI6BZm96t7CgZvH07r3YH6sBClA2o5bI5LhGt\"\n",
    "\n",
    "access_token=\"1000407227130368000-08UrKjmG1TPrLTKqyCsl8VJ9lIEfUp\"\n",
    "access_token_secret=\"brBbDMEEZxuNO2cQ9If1uGzEAeaSHWg83lMYVX2lLdKZj\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "df=pd.read_csv('150425104337_nepal_earthquake_20150425_vol-1.json.csv',sep=',', header=None, index_col=False) \n",
    "# drop the column user_id\n",
    "df = df.drop([1], axis=1)\n",
    "# drop the first row \n",
    "df = df.drop([0])\n",
    "# name the column\n",
    "df.columns = [\"tweet_id\"]  \n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Only the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the lists\n",
    "text=[]\n",
    "deleted_ids=[]\n",
    "requests=0\n",
    "time_rate = datetime.datetime.now()\n",
    "\n",
    "for i in range(len(df)):\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        \n",
    "        print(i)\n",
    "    \n",
    "    requests = requests + 1\n",
    "    \n",
    "    check = datetime.datetime.now()\n",
    "    \n",
    "    if (check - time_rate).seconds > 900 and requests < 850:\n",
    "        \n",
    "        time_rate = check\n",
    "        \n",
    "        requests = 1\n",
    "    \n",
    "    if (check - time_rate).seconds < 900 and requests == 850:\n",
    "        \n",
    "        time.sleep(900 - (check - time_rate).seconds)\n",
    "        \n",
    "        time_rate = check\n",
    "        \n",
    "        requests = 1\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        obj = api.get_status(df.tweet_id[i][1:-1], tweet_mode=\"extended\")\n",
    "    \n",
    "    except tweepy.TweepError:  # in case a tweet is deleted by its user an error will appear\n",
    "                                # I am creating an exception rule so that my code wont crush\n",
    "        deleted_ids.append(df.tweet_id[i])  # I am writing down the location of the missing ids so that I will delete the missing tweets\n",
    "        \n",
    "        continue       \n",
    "    \n",
    "    try:\n",
    "    \n",
    "        text.append(obj.full_text)\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        text.append('NaN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting data from Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the lists\n",
    "\n",
    "favourite_count = []\n",
    "\n",
    "tweet_age = []\n",
    "\n",
    "retweet_count = []\n",
    "\n",
    "profile_age = []\n",
    "\n",
    "user_description = []\n",
    "\n",
    "followers = []\n",
    "\n",
    "friends = []\n",
    "\n",
    "following = []\n",
    "\n",
    "geo_enabled = []\n",
    "\n",
    "extended_profile = []\n",
    "\n",
    "user_location = []\n",
    "\n",
    "name = []\n",
    "\n",
    "profile_image = []\n",
    "\n",
    "statuses_count = []\n",
    "\n",
    "user_time_zone = []\n",
    "\n",
    "verified = []\n",
    "\n",
    "is_translator_enabled = []\n",
    "\n",
    "listed_count = []\n",
    "\n",
    "user_id = []\n",
    "\n",
    "protected = [] \n",
    "\n",
    "screen_name = []\n",
    "\n",
    "time_now = datetime.datetime.now()\n",
    "\n",
    "time_rate = time_now\n",
    "\n",
    "image_exists = []\n",
    "\n",
    "gender_binary = []\n",
    "\n",
    "name_male = []\n",
    "\n",
    "user_lang = []\n",
    "\n",
    "deleted_tweets = []\n",
    "\n",
    "deleted_ids = []\n",
    "\n",
    "notifications = []\n",
    "\n",
    "text=[]\n",
    "\n",
    "default_profile_picture = []\n",
    "\n",
    "requests = 0\n",
    "\n",
    "error_objects = []\n",
    "\n",
    "rumor = []\n",
    "\n",
    "event=[]\n",
    "\n",
    "# define iterative process that will extract information from Twitter\n",
    "for i in range(len(df)):\n",
    "    \n",
    "    # mechanism to control the algorithm from exceeding Twitter's rate limit of\n",
    "    # of 900 requests per 15 minutes\n",
    "    requests = requests + 1\n",
    "    \n",
    "    check = datetime.datetime.now()\n",
    "    \n",
    "    if (check - time_rate).seconds > 900 and requests < 900:\n",
    "        \n",
    "        time_rate = check\n",
    "        \n",
    "        requests = 1\n",
    "    \n",
    "    if (check - time_rate).seconds < 900 and requests == 900:\n",
    "        \n",
    "        time.sleep(900 - (check - time_rate).seconds)\n",
    "        \n",
    "        time_rate = check\n",
    "        \n",
    "        requests = 1\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        \n",
    "        print(i)\n",
    "    \n",
    "    try:\n",
    "        # claim the Tweepy object of the tweet id\n",
    "        obj = api.get_status(df.tweet_id[i], tweet_mode=\"extended\")\n",
    "    \n",
    "    except tweepy.TweepError:  # in case a tweet is deleted by its user an error will appear\n",
    "                                # I am creating an exception rule so that my code wont crush\n",
    "        deleted_ids.append(df.tweet_id[i])  # I am writing down the location of the missing ids so that I will delete the missing tweets\n",
    "        \n",
    "        continue\n",
    "        \n",
    "    rumor.append(df.rumor[i])\n",
    "    \n",
    "    event.append(df.event[i])\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        favourite_count.append(obj.favorite_count) # I count how many users have favoured this tweet\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        favourite_count.append(0)\n",
    "        \n",
    "        error_objects.append(obj)\n",
    "        \n",
    "    try:\n",
    "    \n",
    "        creation_time_tweet = obj.created_at  # I extract the creation time of the tweet\n",
    "    \n",
    "        time_difference = time_now-creation_time_tweet # I count how much time has passed since then\n",
    "    \n",
    "        tweet_age.append(time_difference.seconds) # I turn the difference into seconds\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        tweet_age.append(0)\n",
    "        \n",
    "        error_objects.append(obj)\n",
    "        \n",
    "    try:\n",
    "    \n",
    "        retweet_count.append(obj.retweet_count) # I count the number of re-tweets\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        retweet_count.append(0)\n",
    "        \n",
    "        error_objects.append(obj)\n",
    "        \n",
    "    try:\n",
    "    \n",
    "        user_creation = obj.user.created_at  # I extract the creation time of the user's account\n",
    "    \n",
    "        user_time_difference = time_now-user_creation # I calculate how old is the user's account given the time now\n",
    "    \n",
    "        profile_age.append(user_time_difference.days) # I store the time in days\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        profile_age.append(0)\n",
    "        \n",
    "        error_objects.append(obj)\n",
    "        \n",
    "    try:\n",
    "    \n",
    "        description = obj.user.description # I check if the user has a description for his/her profile or not\n",
    "    \n",
    "        if description == '' :\n",
    "        \n",
    "            user_description.append(0)\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            user_description.append(1)\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        user_description.append(0)\n",
    "        \n",
    "        error_objects.append(obj)\n",
    "        \n",
    "    try:\n",
    "        \n",
    "        text.append(obj.full_text)\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        text.append.append(0)\n",
    "        \n",
    "        error_objects.append(obj)\n",
    "        \n",
    "    try:\n",
    "        \n",
    "        followers.append(obj.user.followers_count) # I count the number of followers of the user\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        followers.append(0)\n",
    "        \n",
    "        error_objects.append(obj)\n",
    "        \n",
    "    try:\n",
    "    \n",
    "        following.append(obj.user.friends_count)  # I count the number of the account that the user is following\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        following.append(0)\n",
    "        \n",
    "        error_objects.append(obj)\n",
    "        \n",
    "    try:\n",
    "    \n",
    "        geo = obj.user.geo_enabled  # I check if the user has enabled the geo_location system of Twitter\n",
    "    \n",
    "        if geo is True:\n",
    "        \n",
    "            geo_enabled.append(1) \n",
    "        \n",
    "        else:\n",
    "        \n",
    "            geo_enabled.append(0)\n",
    "            \n",
    "    except:\n",
    "        \n",
    "        geo_enabled.append(0)\n",
    "        \n",
    "        error_objects.append(obj)\n",
    "        \n",
    "    try:\n",
    "    \n",
    "        extended = obj.user.has_extended_profile # I check if the user has enabled an extended Twitter profile or not\n",
    "    \n",
    "        if extended is True:\n",
    "        \n",
    "            extended_profile.append(1) \n",
    "        \n",
    "        else:\n",
    "        \n",
    "            extended_profile.append(0)\n",
    "            \n",
    "    except:\n",
    "        \n",
    "        extended_profile.append(0)\n",
    "        \n",
    "        error_objects.append(obj)\n",
    "        \n",
    "    try:\n",
    "        \n",
    "        location = obj.user.location  # I check if the user has declared a location or not\n",
    "    \n",
    "        if location == 'NONE':\n",
    "        \n",
    "            user_location.append(0)\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            user_location.append(1)\n",
    "            \n",
    "    except:\n",
    "        \n",
    "        user_location.append(0)\n",
    "        \n",
    "        error_objects.append(obj)\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        statuses_count.append(obj.user.statuses_count) # I count the number of statuses that the user has published\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        statuses_count.append(0)\n",
    "        \n",
    "        error_objects.append(obj)\n",
    "        \n",
    "    try:\n",
    "    \n",
    "        user_notification = obj.user.notifications\n",
    "    \n",
    "        if user_notification is True:\n",
    "        \n",
    "            notifications.append(1)\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            notifications.append(0)\n",
    "            \n",
    "    except:\n",
    "        \n",
    "        notifications.append(0)\n",
    "        \n",
    "        error_objects.append(obj)\n",
    "        \n",
    "    try:\n",
    "    \n",
    "        time_zone = obj.user.time_zone # I check if the user has declared a timezone\n",
    "    \n",
    "        if time_zone == 'None':\n",
    "        \n",
    "            user_time_zone.append(0)\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            user_time_zone.append(1)\n",
    "            \n",
    "    except:\n",
    "        \n",
    "        user_time_zone.append(0)\n",
    "        \n",
    "        error_objects.append(obj)\n",
    "        \n",
    "    try:\n",
    "        \n",
    "        verification = obj.user.verified # I check if the user has verified his/her account or not\n",
    "    \n",
    "        if verification is True:\n",
    "        \n",
    "            verified.append(1)\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            verified.append(0)\n",
    "            \n",
    "    except:\n",
    "        \n",
    "        verified.append(0)\n",
    "        \n",
    "        error_objects.append(obj)\n",
    "        \n",
    "    try:\n",
    "    \n",
    "        pic_default = obj.user.default_profile_image # I check if the user has verified his/her account or not\n",
    "    \n",
    "        if pic_default is True:\n",
    "        \n",
    "            default_profile_picture.append(1)\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            default_profile_picture.append(0)\n",
    "            \n",
    "    except:\n",
    "        \n",
    "        default_profile_picture.append(0)\n",
    "        \n",
    "        error_objects.append(obj)\n",
    "        \n",
    "    try:\n",
    "    \n",
    "        translation = obj.user.is_translation_enabled  # I check if the user has enabled the Twitter's translator\n",
    "    \n",
    "        if translation is True:\n",
    "        \n",
    "            is_translator_enabled.append(1)\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            is_translator_enabled.append(0)\n",
    "            \n",
    "    except:\n",
    "        \n",
    "        is_translator_enabled.append(0)\n",
    "        \n",
    "        error_objects.append(obj)\n",
    "        \n",
    "    try:\n",
    "        \n",
    "        listed_count.append(obj.user.listed_count)  # I count the number of lists that the user has\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        listed_count.append(0)\n",
    "        \n",
    "        error_objects.append(obj)\n",
    "        \n",
    "    try:\n",
    "    \n",
    "        protection = obj.user.protected   # I check if the account of the user is protected or not\n",
    "    \n",
    "        if protection is True:\n",
    "        \n",
    "            protected.append(1)\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            protected.append(0)\n",
    "            \n",
    "    except:\n",
    "        \n",
    "        protected.append(0)\n",
    "        \n",
    "        error_objects.append(obj)\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        image=urllib.request.urlretrieve(obj.user.profile_image_url,'1.jpg')   # I download the profile picture of the user \n",
    "    \n",
    "        image = face_recognition.load_image_file(\"1.jpg\")  # I turn the picture into an object of the Face Recognition Library\n",
    "\n",
    "        face_locations=face_recognition.face_locations(image) # I use the face recognition library to locate if there are any clear faces in the picture\n",
    "    \n",
    "        if face_locations==[]:\n",
    "        \n",
    "            image_exists.append(0)\n",
    "    \n",
    "        else:\n",
    "        \n",
    "            image_exists.append(1)\n",
    "            \n",
    "    except:\n",
    "        \n",
    "        image_exists.append(0)\n",
    "        \n",
    "        error_objects.append(obj)\n",
    "        \n",
    "    try:\n",
    "        \n",
    "        d = gender.Detector()\n",
    "    \n",
    "        name.append(obj.user.name)  # I extract the name of the user  \n",
    "    \n",
    "        sex = d.get_gender(str(name[i]).split()[0]) # I use the name of the user to define the user's gender\n",
    "    \n",
    "        if sex=='unknown':  # check if the name indicates a gender or not\n",
    "        \n",
    "            gender_binary.append(0)\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            gender_binary.append(1)\n",
    "        \n",
    "        if sex is 'male' or 'mostly_male': # check if the user is male or not\n",
    "        \n",
    "            name_male.append(1)\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            name_male.append(0)\n",
    "            \n",
    "    except:\n",
    "        \n",
    "        gender_binary.append(0)\n",
    "        \n",
    "        name_male.append(0)\n",
    "        \n",
    "        error_objects.append(obj)\n",
    "        \n",
    "    language=obj.user.lang # extract the language of the user    \n",
    "    \n",
    "    user_lang.append(language)\n",
    "\n",
    "    screen_name.append(obj.user.screen_name)  # I extract the screen name of the user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Additional Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweet_per_second = [ retweet_count[i]/tweet_age[i] for i in range(0,len(retweet_count))]\n",
    "    \n",
    "favourite_per_second = [ favourite_count[i]/tweet_age[i] for i in range(0,len(tweet_age))]\n",
    "    \n",
    "followers_per_day = [ followers[i]/profile_age[i] for i in range(0,len(followers))]\n",
    "\n",
    "following_per_day = [ following[i]/profile_age[i] for i in range(0,len(following))]\n",
    "    \n",
    "followers_following_ratio=[]\n",
    "\n",
    "for i in range(len(followers)):\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        if following[i] == 0:\n",
    "        \n",
    "            following[i] = 1.0\n",
    "    \n",
    "        a=(followers[i] / following[i])\n",
    "    \n",
    "        followers_following_ratio.append(a)\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        followers_following_ratio.append(0)\n",
    "        \n",
    "        error_objects.append(obj)\n",
    "        \n",
    "statuses_per_day = [ statuses_count[i] / profile_age[i] for i in range(len(profile_age))]\n",
    "\n",
    "retweet_per_follower = [ retweet_count[i] / following[i] for i in range(len(followers))]\n",
    "\n",
    "favourite_per_follower = [ favourite_count[i] / following[i] for i in range(len(following))]\n",
    "\n",
    "lists_per_day = [ listed_count[i] / profile_age[i] for i in range(len(profile_age))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting the missing Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(deleted_ids)):\n",
    "        \n",
    "    df = df[df.tweet_id != deleted_ids[i]] # I am keeping only the values that do not match with the deleted Tweets\n",
    "    \n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataframes in order to extract the desired features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.Series(list(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating features from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters=df2.str.len()             # I count the number of characters within a tweet.\n",
    "\n",
    "exclamation_mark = df2.str.count('!')  # I count the number of exclamation marks.\n",
    "\n",
    "commas = df2.str.count(',')           # I count the number of commas.\n",
    "\n",
    "periods = df2.str.count('.')        # I count the number of periods.\n",
    "\n",
    "question_mark = df2.str.count('\\?')    # I count the number of question marks.\n",
    "\n",
    "hashtag = df2.str.count('#')          # I count the number of hashtags.\n",
    "\n",
    "url = df2.str.count('http')           # I count the number of urls.\n",
    "\n",
    "mention = df2.str.count('@')          # I count the number of mentions.\n",
    "\n",
    "words = df2.str.count(' ')+1          # I count the number of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if there are multiple dictation marks in the Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_marks=[]\n",
    "\n",
    "for i in range(len(exclamation_mark)):\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        if exclamation_mark[i]>0 and question_mark[i]>0 : # check if the question marks and the exclamation marks are above zero\n",
    "        \n",
    "            multiple_marks.append(1) # in case there are write 1\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            multiple_marks.append(0) # in case there are not write 0\n",
    "            \n",
    "    except:\n",
    "        \n",
    "        multiple_marks.append(0)\n",
    "        \n",
    "        error_objects.append(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check how accurate is the dictation of the Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictation_rate=[]\n",
    "\n",
    "for i in range(len(df2)):\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        a=str(df2[i])        # define the tweet as a string variable\n",
    "        \n",
    "        b=TextBlob(str((df2)[i]))  # turn the tweet into a textblob object\n",
    "    \n",
    "        b=b.correct()   # correct the dictation of the words\n",
    "        \n",
    "        dictation=SequenceMatcher(None, a, b).ratio()   # compare the matching of the two strings\n",
    "    \n",
    "        dictation_rate.append(dictation)  # append the matching to the dictation_rate list\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        dictation_rate.append(0)\n",
    "        \n",
    "        error_objects.append(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Files with sets of positive and negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = []\n",
    "\n",
    "with open(\"positive-words.txt\") as file:\n",
    "    \n",
    "    for line in file: \n",
    "        \n",
    "        line = line.strip() \n",
    "        \n",
    "        positive.append(line) # create a list with the positive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative = []\n",
    "\n",
    "with open(\"negative-words.txt\") as file:\n",
    "    \n",
    "    for line in file: \n",
    "        \n",
    "        line = line.strip() \n",
    "        \n",
    "        negative.append(line) # create a list with the negative words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check how many positive and negative words the Tweet entail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "positive_words=[]\n",
    "\n",
    "for i in range(len(df2)):\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        test=(str(df2[i]))\n",
    "    \n",
    "        positives = [char for char in test if char in positive] # I create a list of the matching words\n",
    "    \n",
    "        positive_words.append(len(positives)) # I append the length of the list created\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        positive_words.append(0)\n",
    "        \n",
    "        error_objects.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_words=[]\n",
    "\n",
    "for i in range(len(df2)):\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        test=(str(df2[i]))\n",
    "    \n",
    "        negatives = [char for char in test if char in negative]   # I create a list of the matching words\n",
    "    \n",
    "        negative_words.append(len(negatives))   # I append the length of the list created\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        negative_words.append(0)\n",
    "        \n",
    "        error_objects.append(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use TextBlob Library to analyze the text of the Tweet and calculate the subjectivity and the polarity of the Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjectivity=[]\n",
    "\n",
    "for i in range(len(df2)):\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        test=TextBlob(str(df2[i])) # I turn the text of the tweet into a textblob object\n",
    "    \n",
    "        subjectivity.append(test.sentiment.subjectivity) # I append the subjectivity score to the subjectivity list\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        subjectivity.append(0)\n",
    "        \n",
    "        error_objects.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarity=[]\n",
    "\n",
    "for i in range(len(df2)):\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        test=TextBlob(str(df2[i])) # I turn the text of the tweet into a textblob object\n",
    "    \n",
    "        polarity.append(test.sentiment.polarity) # I append the polarity score to the polarity list\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        polarity.append(0)\n",
    "        \n",
    "        error_objects.append(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the number of uppercase letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "uppercase_letters=[]\n",
    "\n",
    "for i in range(len(df2)):\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        a=(str(df2[i])).split() # I break the tweet string into words\n",
    " \n",
    "        count = 0\n",
    "\n",
    "        for word in a:  \n",
    "        \n",
    "            for letter in word:  # With a double loop I check each letter of the string\n",
    "            \n",
    "                if letter.isupper():  # if a letter is uppercase\n",
    "            \n",
    "                    count = count + 1\n",
    "            \n",
    "        uppercase_letters.append(count)\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        uppercase_letters.append(0)\n",
    "        \n",
    "        error_objects.append(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a list of the unique hashtags in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_list=[]\n",
    "\n",
    "list_of_hashtags = []\n",
    "\n",
    "for i in range(len(df2)):\n",
    "    \n",
    "    tweet_hashtags = []\n",
    "    \n",
    "    a=(str(df2[i])).split() # I break the tweet string into words\n",
    "    \n",
    "    for word in a:\n",
    "    \n",
    "        if word.startswith('#'):  # if the word starts with # is a hashtag\n",
    "        \n",
    "            tweet_hashtags.append(word)\n",
    "            \n",
    "            list_of_hashtags.append(word)\n",
    "            \n",
    "    hashtag_list.append(tweet_hashtags)\n",
    "        \n",
    "list_of_hashtags = list(set(list_of_hashtags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"generic2_hastags.txt\", \"w\") as f:\n",
    "    \n",
    "    for s in hashtag_list:\n",
    "        \n",
    "        f.write(str(s) +\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the number of places that are mentioned in a Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "places=[]\n",
    "\n",
    "for i in range(len(df2)):\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        a=GeoText(str(df2[i])) # I turn the string of the tweet into a Geotext object\n",
    "    \n",
    "        a=a.cities # I create a dictionary with the cities/places that are mentioned in the text\n",
    "    \n",
    "        places.append(len(a)) # I append the length of the dictionary to the places list\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        places.append(0)\n",
    "        \n",
    "        error_objects.append(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a list of the unique places that are in the dataset using the GeoText library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_list=[]\n",
    "\n",
    "list_of_places = []\n",
    "\n",
    "for i in range(len(df2)):\n",
    "    \n",
    "    a=GeoText(str(df2[i])) # I turn the text into an Geotext object\n",
    "    \n",
    "    a=a.cities\n",
    "    \n",
    "    places_list.append(a)\n",
    "    \n",
    "    for i in range(len(a)):\n",
    "        \n",
    "        if a[i]!=[]:  # if the tweet entails any cities/places they are appended into the places_list\n",
    "            \n",
    "            list_of_places.append(a[i])\n",
    "        \n",
    "list_of_places = list(set(list_of_places))  # I create a list with the unique places in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"generic2_places.txt\", \"w\") as f:\n",
    "    \n",
    "    for s in places_list:\n",
    "        \n",
    "        f.write(str(s) +\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the number of times the First, Second and Third person pronoun is used in a Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_pronoun=[]\n",
    "\n",
    "first = ['i','me','myself','mine','we','us','ours','ourselves'] # a list of the first person pronouns\n",
    "\n",
    "for i in range(len(df2)):\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        count=0\n",
    "    \n",
    "        a = str(df2[i]).split() # I break the tweet into words\n",
    "    \n",
    "        for i in range(len(a)):\n",
    "        \n",
    "            if a[i].lower() in first: #if a word is a first person pronoun\n",
    "            \n",
    "                count = count + 1\n",
    "            \n",
    "            first_pronoun.append(count)\n",
    "            \n",
    "    except:\n",
    "        \n",
    "        first_pronoun.append(0)\n",
    "        \n",
    "        error_objects.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_pronoun=[]\n",
    "\n",
    "second = ['you','yours','yourself','yourselves']\n",
    "\n",
    "for i in range(len(df2)):\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        count=0\n",
    "    \n",
    "        a = str(df2[i]).split(' ')\n",
    "    \n",
    "        for i in range(len(a)):\n",
    "        \n",
    "            if a[i].lower() in second:\n",
    "            \n",
    "                count = count + 1\n",
    "            \n",
    "            second_pronoun.append(count)\n",
    "            \n",
    "    except:\n",
    "        \n",
    "        second_pronoun.append(0)\n",
    "        \n",
    "        error_objects.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_pronoun=[]\n",
    "\n",
    "third = ['he','she','it','him','her','his','hers','himself','herself','itself','they','them','theirs','themselves']\n",
    "\n",
    "for i in range(len(df2)):\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        count=0\n",
    "    \n",
    "        a = str(df2[i]).split(' ')\n",
    "    \n",
    "        for i in range(len(a)):\n",
    "        \n",
    "            if a[i].lower() in third:\n",
    "            \n",
    "                count = count + 1\n",
    "            \n",
    "            third_pronoun.append(count)\n",
    "            \n",
    "    except:\n",
    "        \n",
    "        third_pronoun.append(0)\n",
    "        \n",
    "        error_objects.append(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for words that indicate the author of the tweet is an eye-witness of the event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "witness_claim=[]\n",
    "\n",
    "witness_words=['see','saw','seen', 'attend', 'attended', 'notice', 'noticed', 'watch', 'watched',\n",
    "              'bystander', 'eyewitness', 'observer', 'recognize', 'recognise', 'recognized', 'recognised',\n",
    "              'witness', 'witnessed', 'follow', 'followed', 'observe', 'observed']\n",
    "# words that indicate that the author of the tweet is/was a witness of the event\n",
    "\n",
    "for i in range(len(df2)):  # the process is the same as before with me counting the matching \n",
    "                            #words of the tweet to a list of words\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        count=0\n",
    "    \n",
    "        a=str(df2[i]).split(' ')\n",
    "    \n",
    "        for i in range(len(a)):     \n",
    "        \n",
    "            if a[i].lower() in witness_words:\n",
    "            \n",
    "                    count = count + 1\n",
    "            \n",
    "            witness_claim.append(count)  \n",
    "            \n",
    "    except:\n",
    "        \n",
    "        witness_claim.append(0)\n",
    "        \n",
    "        error_objects.append(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching of user's language and the language of the Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_match=[]\n",
    "\n",
    "text_lang=[]\n",
    "\n",
    "for i in range(len(df2)):\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        a = TextBlob(str(df2[i]))\n",
    "    \n",
    "        text_lang.append(a.detect_language())\n",
    "\n",
    "        for i in range(len(user_lang)): \n",
    "    \n",
    "            if user_lang[i] == text_lang[i]: # check if the language of the text and the user match\n",
    "        \n",
    "                lang_match.append(1)\n",
    "        \n",
    "            else:\n",
    "        \n",
    "                lang_match.append(0)\n",
    "            \n",
    "    except:\n",
    "        \n",
    "        lang_match.append(0)\n",
    "        \n",
    "        error_objects.append(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if the name of the user indicates that the user is an opinion shaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_shaper=[]\n",
    "\n",
    "for i in range(len(name)):\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        count=0\n",
    "    \n",
    "        b=str(screen_name[i]).lower()\n",
    "        \n",
    "        opinion=['news','breaking','news','report','report','daily','times', 'feed', 'radar', 'net']\n",
    "            \n",
    "        for i in range(len(opinion)):\n",
    "            \n",
    "            if opinion[i] in b:\n",
    "        \n",
    "                count = count +1\n",
    "    \n",
    "        if count>0:\n",
    "        \n",
    "            opinion_shaper.append(1)\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            opinion_shaper.append(0)\n",
    "            \n",
    "    except:\n",
    "        \n",
    "        opinion_shaper.append(0)\n",
    "        \n",
    "        error_objects.append(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a dataframe with the features created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Networking Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "metacontent_features = pd.DataFrame(list(zip(favourite_count, tweet_age, retweet_count, mention, \n",
    "                                         retweet_per_second, favourite_per_second,\n",
    "                                        retweet_per_follower, favourite_per_follower)),\n",
    "                            columns = ['favourite_count', 'tweet_age', 'retweet_count', 'mentions', 'retweet_per_second'\n",
    "                                       ,'favourite_per_second','retweet_per_follower', 'favourite_per_follower'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User profile Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features = pd.DataFrame(list(zip(profile_age, user_description, geo_enabled, opinion_shaper,extended_profile,\n",
    "                                      user_location, statuses_count, user_time_zone, verified, is_translator_enabled,\n",
    "                                      listed_count, protected, notifications, default_profile_picture, image_exists, \n",
    "                                      gender_binary, name_male, statuses_per_day,lists_per_day, followers, \n",
    "                                      following, followers_per_day, following_per_day, followers_following_ratio)),\n",
    "                             columns = ['profile_age', 'profile_description', 'geolocation', 'opinion_shaper',\n",
    "                                        'extended_profile','user_location', 'statuses_count', 'user_timezone', \n",
    "                                        'verified_account','text_translator', 'user_lists', 'account_protection', \n",
    "                                        'notifications','default_profile_picture', 'profile_picture', 'gender', \n",
    "                                        'male', 'statuses_per_day', 'lists_per_day','followers', \n",
    "                                        'following', 'followers_per_day', 'following_per_day', \n",
    "                                        'followers_following_ratio'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweet text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "linguistic_features = pd.DataFrame(list(zip(characters, exclamation_mark, question_mark, hashtag, url, words,\n",
    "                                      commas, periods, multiple_marks,lang_match, dictation_rate, subjectivity,\n",
    "                                        polarity, positive_words, negative_words, uppercase_letters, places, \n",
    "                                        first_pronoun, second_pronoun, third_pronoun, witness_claim)),\n",
    "                             columns = ['characters', 'exclamation_marks', 'question_marks', 'hashtags', 'urls',\n",
    "                                        'words', 'commas', 'periods', 'multiple_marks','lang_match', \n",
    "                                        'dictation_rate', 'subjectivity', 'polarity', 'positive_words', \n",
    "                                        'negative_words', 'uppercase_letters', 'places', 'first_person',\n",
    "                                        'second_person', 'third_person', 'witness'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the values of the features extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = metacontent_features.values\n",
    "\n",
    "y = user_features.values\n",
    "\n",
    "z = linguistic_features.values\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "\n",
    "y_scaled = min_max_scaler.fit_transform(y)\n",
    "\n",
    "z_scaled = min_max_scaler.fit_transform(z)\n",
    "\n",
    "metacontent_features = pd.DataFrame(x_scaled, \n",
    "                               columns = ['favourite_count', 'tweet_age', 'retweet_count', 'mentions', \n",
    "                                          'retweet_per_second', 'favourite_per_second','retweet_per_follower',\n",
    "                                          'favourite_per_follower'])\n",
    "\n",
    "user_features = pd.DataFrame(y_scaled,\n",
    "                            columns = ['profile_age', 'profile_description', 'geolocation', 'opinion_shaper',\n",
    "                                        'extended_profile','user_location', 'statuses_count', 'user_timezone', \n",
    "                                        'verified_account','text_translator', 'user_lists', 'account_protection', \n",
    "                                        'notifications','default_profile_picture', 'profile_picture', 'gender', \n",
    "                                        'male', 'statuses_per_day', 'lists_per_day','followers', \n",
    "                                        'following', 'followers_per_day', 'following_per_day', \n",
    "                                       'followers_following_ratio'])\n",
    "\n",
    "linguistic_features = pd.DataFrame(z_scaled,\n",
    "                            columns = ['characters', 'exclamation_marks', 'question_marks', 'hashtags', 'urls',\n",
    "                                        'words', 'commas', 'periods', 'multiple_marks','lang_match', \n",
    "                                        'dictation_rate', 'subjectivity', 'polarity', 'positive_words', 'negative_words',\n",
    "                                       'uppercase_letters', 'places', 'first_person', 'second_person', 'third_person', \n",
    "                                       'witness'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>characters</th>\n",
       "      <th>exclamation_marks</th>\n",
       "      <th>question_marks</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>urls</th>\n",
       "      <th>words</th>\n",
       "      <th>commas</th>\n",
       "      <th>periods</th>\n",
       "      <th>multiple_marks</th>\n",
       "      <th>lang_match</th>\n",
       "      <th>...</th>\n",
       "      <th>following</th>\n",
       "      <th>mentions</th>\n",
       "      <th>retweet_per_second</th>\n",
       "      <th>favoutire_per_second</th>\n",
       "      <th>followers_per_day</th>\n",
       "      <th>following_per_day</th>\n",
       "      <th>followers_following_ratio</th>\n",
       "      <th>retweet_per_follower</th>\n",
       "      <th>favourite_per_follower</th>\n",
       "      <th>rumor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.453390</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.276316</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.453390</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003558</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.262865e-06</td>\n",
       "      <td>0.003581</td>\n",
       "      <td>5.654038e-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.338983</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.144737</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338983</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.036065e-06</td>\n",
       "      <td>0.003012</td>\n",
       "      <td>2.185529e-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.457627</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.184211</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.457627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.187493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009991</td>\n",
       "      <td>3.152742e-03</td>\n",
       "      <td>0.151742</td>\n",
       "      <td>6.716987e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.398305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.171053</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.398305</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.956693e-06</td>\n",
       "      <td>0.004289</td>\n",
       "      <td>1.474816e-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000430</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.209657e-07</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>6.540912e-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   characters  exclamation_marks  question_marks  hashtags  urls     words  \\\n",
       "0    0.453390           0.086957        0.000000  0.000000  0.00  0.276316   \n",
       "1    0.338983           0.043478        0.000000  0.000000  0.25  0.144737   \n",
       "2    0.457627           0.000000        0.000000  0.153846  0.25  0.184211   \n",
       "3    0.398305           0.000000        0.083333  0.000000  0.00  0.171053   \n",
       "4    0.067797           0.000000        0.000000  0.000000  0.25  0.000000   \n",
       "\n",
       "     commas   periods  multiple_marks  lang_match  ...    following  mentions  \\\n",
       "0  0.090909  0.453390             0.0         1.0  ...     0.003558       0.0   \n",
       "1  0.000000  0.338983             0.0         0.0  ...     0.002989       0.0   \n",
       "2  0.090909  0.457627             0.0         1.0  ...     0.187493       0.0   \n",
       "3  0.272727  0.398305             0.0         0.0  ...     0.004333       0.0   \n",
       "4  0.000000  0.067797             0.0         0.0  ...     0.000430       0.0   \n",
       "\n",
       "   retweet_per_second  favoutire_per_second  followers_per_day  \\\n",
       "0                 0.0              0.000000       6.262865e-06   \n",
       "1                 0.0              0.000000       2.036065e-06   \n",
       "2                 0.0              0.009991       3.152742e-03   \n",
       "3                 0.0              0.000000       1.956693e-06   \n",
       "4                 0.0              0.000000       8.209657e-07   \n",
       "\n",
       "   following_per_day  followers_following_ratio  retweet_per_follower  \\\n",
       "0           0.003581               5.654038e-08                   0.0   \n",
       "1           0.003012               2.185529e-08                   0.0   \n",
       "2           0.151742               6.716987e-07                   0.0   \n",
       "3           0.004289               1.474816e-08                   0.0   \n",
       "4           0.000406               6.540912e-08                   0.0   \n",
       "\n",
       "   favourite_per_follower  rumor  \n",
       "0                 0.00000   True  \n",
       "1                 0.00000   True  \n",
       "2                 0.00005   True  \n",
       "3                 0.00000   True  \n",
       "4                 0.00000   True  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = pd.concat([metacontent_features,user_features,linguistic_features], axis=1)\n",
    "\n",
    "features = pd.DataFrame(features, \n",
    "                        columns = ['favourite_count', 'tweet_age', 'retweet_count', 'mentions', \n",
    "                                          'retweet_per_second', 'favourite_per_second','retweet_per_follower',\n",
    "                                          'favourite_per_follower','profile_age', 'profile_description', \n",
    "                                   'geolocation', 'opinion_shaper', 'extended_profile','user_location', \n",
    "                                   'statuses_count', 'user_timezone', \n",
    "                                        'verified_account','text_translator', 'user_lists', 'account_protection', \n",
    "                                        'notifications','default_profile_picture', 'profile_picture', 'gender', \n",
    "                                        'male', 'statuses_per_day', 'lists_per_day','followers', \n",
    "                                        'following', 'followers_per_day', 'following_per_day', \n",
    "                                       'followers_following_ratio', 'characters', 'exclamation_marks', 'question_marks', \n",
    "                                   'hashtags', 'urls', 'words', 'commas', 'periods', 'multiple_marks','lang_match', \n",
    "                                        'dictation_rate', 'subjectivity', 'polarity', 'positive_words', 'negative_words',\n",
    "                                       'uppercase_letters', 'places', 'first_person', 'second_person', 'third_person', \n",
    "                                       'witness'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.to_csv('extracted.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Classification Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = pd.DataFrame(list(zip(text)), columns= ['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.to_csv('earthquake(8651_85000).csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(mess):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunc = [char for char in mess if char not in string.punctuation]\n",
    "\n",
    "    # Join the characters again to form the string.\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    # Now just remove any stopwords\n",
    "    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.Text = dff.Text.apply(text_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.to_csv('dffprocessed.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_transformer = CountVectorizer(analyzer=text_process).fit(dff.Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_bow = bow_transformer.transform(dff.Text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
