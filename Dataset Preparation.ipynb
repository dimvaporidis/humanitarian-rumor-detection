{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding that function which will create the data per event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def claim_tweet_ids(x,a,b):\n",
    "    \n",
    "    df=pd.read_csv(x,sep='\\t', header=None, index_col=False, error_bad_lines=False)\n",
    "    \n",
    "    df.columns = [\"number\", \"tweet_id\", \"text\",'time']  # name the columns\n",
    "    \n",
    "    del df['text']\n",
    "    \n",
    "    del df['number']\n",
    "    \n",
    "    del df['time']\n",
    "    \n",
    "    df['Rumor'] = b\n",
    "    \n",
    "    if a == 1 :\n",
    "        \n",
    "        df['True'] = True\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        df['True'] = False\n",
    "        \n",
    "    train, test = train_test_split(df, test_size=0.25)\n",
    "        \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call the function per event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 21: expected 4 fields, saw 8\\n'\n",
      "b'Skipping line 2707: expected 4 fields, saw 5\\n'\n",
      "b'Skipping line 101: expected 4 fields, saw 5\\n'\n",
      "b'Skipping line 247: expected 4 fields, saw 24\\n'\n"
     ]
    }
   ],
   "source": [
    "train1, test1 = claim_tweet_ids('N_Airliner.txt', 1, 1)\n",
    "\n",
    "train2, test2 = claim_tweet_ids('N_AnnieLe.txt', 1, 2)\n",
    "\n",
    "train3, test3 = claim_tweet_ids('N_BeefProtest.txt', 1, 3)\n",
    "\n",
    "train4, test4 = claim_tweet_ids('N_ByrdBillings.txt', 1, 4)\n",
    "\n",
    "train5, test5 = claim_tweet_ids('N_ChristianTheLion.txt', 1, 5)\n",
    "\n",
    "train6, test6 = claim_tweet_ids('N_Cristiano.txt', 1, 6)\n",
    "\n",
    "train7, test7 = claim_tweet_ids('N_District.txt', 1, 7)\n",
    "\n",
    "train8, test8 = claim_tweet_ids('N_Englandback.txt', 1, 8)\n",
    "\n",
    "train9, test9 = claim_tweet_ids('N_Giantcoconutcrab.txt', 1, 9)\n",
    "\n",
    "train10, test10 = claim_tweet_ids('N_HamsterOnAPiano.txt', 1, 10)\n",
    "\n",
    "train11, test11 = claim_tweet_ids('R_AsparagusCancer.txt', 0, 11)\n",
    "\n",
    "train12, test12 = claim_tweet_ids('R_Bigfoot.txt', 0, 12)\n",
    "\n",
    "train13, test13 = claim_tweet_ids('R_Carmen.txt', 0, 13)\n",
    "\n",
    "train14, test14 = claim_tweet_ids('R_CellphoneGasExplosion.txt', 0, 14)\n",
    "\n",
    "train15, test15 = claim_tweet_ids('R_Chupacabra.txt', 0, 15)\n",
    "\n",
    "train16, test16 = claim_tweet_ids('R_CookCellphone.txt', 0, 16)\n",
    "\n",
    "train17, test17 = claim_tweet_ids('R_DennisKucinichUFO.txt', 0, 17)\n",
    "\n",
    "train18, test18 = claim_tweet_ids('R_DietCokeBacon.txt', 0, 18)\n",
    "\n",
    "train19, test19 = claim_tweet_ids('R_Duckquack.txt', 0, 19)\n",
    "\n",
    "train20, test20 = claim_tweet_ids('R_GiantCatfish.txt', 0, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging the data into 2 frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = shuffle(pd.concat([train1, train2, train3, train4, train5, train6, train7, train8, train9, train10, \n",
    "                        train11, train12, train13, train14, train15, train16, train17, train18, train19, train20]))\n",
    "\n",
    "testing_data = shuffle(pd.concat([test1, test2, test3, test4, test5, test6, test7, test8, test9, test10, test11, test12, \n",
    "                          test13, test14, test15, test16, test17, test18, test19, test20]))\n",
    "\n",
    "extracting_data = pd.concat([training_data,testing_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52678"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extracting_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracting_data=pd.concat([train5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2928"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extracting_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the 2 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_data.to_csv('training_data.csv', sep='\\t')\n",
    "\n",
    "#testing_data.to_csv('testing_data.csv', sep='\\t')\n",
    "\n",
    "extracting_data.to_csv('extracting_data.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
